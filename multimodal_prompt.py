#!/usr/bin/env python3
"""Send an image + dynamic prompt to a ModelScope endpoint via the OpenAI client."""

from __future__ import annotations

import argparse
import base64
import mimetypes
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Generator, List

from openai import OpenAI


DEFAULT_MODEL = os.getenv("MODELSCOPE_MODEL") or "Qwen/Qwen3-VL-235B-A22B-Instruct"
DEFAULT_BASE_URL = os.getenv("MODELSCOPE_BASE_URL") or "https://api-inference.modelscope.cn/v1"
DEFAULT_SYSTEM_PROMPT = "你是一个CAD工程图纸识别的助手，你的任务是分析给定的图纸，给出专业的数值。"


@dataclass
class Arguments:
    images: List[Path]
    prompt: str
    model: str
    base_url: str
    api_key: str
    system_prompt: str
    temperature: float
    max_tokens: int
    stream: bool
    preview_prompt: bool


def parse_arguments() -> Arguments:
    parser = argparse.ArgumentParser(
        description=(
            "Format a dynamic prompt, attach an image, and call a ModelScope model "
            "via its OpenAI-compatible API."
        )
    )
    parser.add_argument(
        "images",
        type=Path,
        nargs="+",
        help="Path(s) to one or more image files to send with the prompt.",
    )

    prompt_group = parser.add_mutually_exclusive_group(required=True)
    prompt_group.add_argument(
        "--prompt",
        help="Prompt text entered directly on the command line.",
    )
    prompt_group.add_argument(
        "--prompt-file",
        type=Path,
        help="Read the prompt template from a text file.",
    )

    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL,
        help=(
            "ModelScope model id (e.g., Qwen/Qwen3-VL-235B-A22B-Instruct). "
            "Defaults to the official Qwen3 VL reference model."
        ),
    )
    parser.add_argument(
        "--base-url",
        default=DEFAULT_BASE_URL,
        help="Base URL for the ModelScope OpenAI-compatible endpoint.",
    )
    parser.add_argument(
        "--api-key",
        help=(
            "ModelScope token. If omitted, the script uses MODELSCOPE_API_KEY, "
            "MODELSCOPE_TOKEN, or OPENAI_API_KEY environment variables in that order."
        ),
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.2,
        help="Sampling temperature passed to the model. Defaults to 0.2.",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=2048,
        help="Maximum number of tokens generated by the assistant. Defaults to 2048.",
    )
    parser.add_argument(
        "--system-prompt",
        default=DEFAULT_SYSTEM_PROMPT,
        help="System prompt injected ahead of each request. Defaults to a CAD-analysis instruction.",
    )
    parser.add_argument(
        "--stream",
        action="store_true",
        help="Stream tokens as they arrive (prints to stdout immediately).",
    )
    parser.add_argument(
        "--preview-prompt",
        action="store_true",
        help="Print the final prompt before calling the model.",
    )

    args = parser.parse_args()

    prompt_text = args.prompt
    if args.prompt_file:
        prompt_text = args.prompt_file.read_text(encoding="utf-8")

    resolved_api_key = (
        args.api_key
        or os.getenv("MODELSCOPE_API_KEY")
        or os.getenv("MODELSCOPE_TOKEN")
        or os.getenv("OPENAI_API_KEY")
    )
    if not resolved_api_key:
        parser.error(
            "API key missing: use --api-key or set MODELSCOPE_API_KEY / MODELSCOPE_TOKEN / OPENAI_API_KEY."
        )

    return Arguments(
        images=args.images,
        prompt=prompt_text,
        model=args.model,
        base_url=args.base_url,
        api_key=resolved_api_key,
        system_prompt=args.system_prompt,
        temperature=args.temperature,
        max_tokens=args.max_tokens,
        stream=args.stream,
        preview_prompt=args.preview_prompt,
    )


def encode_image(image_path: Path) -> tuple[str, str]:
    image_bytes = image_path.read_bytes()
    image_b64 = base64.b64encode(image_bytes).decode("utf-8")
    mime_type, _ = mimetypes.guess_type(image_path.as_posix())
    return image_b64, mime_type or "application/octet-stream"


def build_chat_messages(system_prompt: str, prompt: str, image_paths: List[Path]) -> List[dict]:
    contents = [{"type": "text", "text": prompt}]
    for image_path in image_paths:
        image_b64, mime_type = encode_image(image_path)
        data_url = f"data:{mime_type};base64,{image_b64}"
        contents.append({"type": "image_url", "image_url": {"url": data_url}})
    return [
        {
            "role": "system",
            "content": [{"type": "text", "text": system_prompt}],
        },
        {
            "role": "user",
            "content": contents,
        },
    ]


def extract_text_chunks(content: Any) -> List[str]:
    chunks: List[str] = []
    if isinstance(content, str):
        chunks.append(content)
    elif isinstance(content, list):
        for block in content:
            if isinstance(block, dict):
                text = block.get("text")
            else:
                text = getattr(block, "text", None)
            if text:
                chunks.append(str(text))
    elif isinstance(content, dict):
        text = content.get("text")
        if text:
            chunks.append(str(text))
    return chunks


def extract_text_output(response) -> str:
    choices = getattr(response, "choices", None) or []
    if not choices:
        return ""

    first_choice = choices[0]
    message = getattr(first_choice, "message", None)
    if message is None:
        return ""

    content = getattr(message, "content", None)
    chunks = extract_text_chunks(content)
    return "\n".join(chunk for chunk in chunks if chunk).strip()


def call_modelscope_endpoint(
    base_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    prompt: str,
    image_paths: List[Path],
    temperature: float,
    max_tokens: int,
) -> str:
    client = OpenAI(base_url=base_url, api_key=api_key)
    messages = build_chat_messages(system_prompt, prompt, image_paths)
    response = client.chat.completions.create(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        messages=messages,
    )
    return extract_text_output(response)


def stream_modelscope_endpoint(
    base_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    prompt: str,
    image_paths: List[Path],
    temperature: float,
    max_tokens: int,
) -> Generator[str, None, None]:
    client = OpenAI(base_url=base_url, api_key=api_key)
    messages = build_chat_messages(system_prompt, prompt, image_paths)
    stream = client.chat.completions.create(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        messages=messages,
        stream=True,
    )

    for chunk in stream:
        choices = getattr(chunk, "choices", None) or []
        if not choices:
            continue
        delta = getattr(choices[0], "delta", None)
        if delta is None:
            continue
        content = getattr(delta, "content", None)
        chunks = extract_text_chunks(content)
        if not chunks:
            continue
        text = "".join(chunks)
        if text:
            yield text


def main() -> None:
    args = parse_arguments()

    missing = [path for path in args.images if not path.exists()]
    if missing:
        missing_str = ", ".join(str(path) for path in missing)
        raise SystemExit(f"Image file(s) not found: {missing_str}")

    final_prompt = args.prompt.strip()

    if args.preview_prompt:
        print("Final prompt:\n" + final_prompt)

    if args.stream:
        combined = ""
        for delta in stream_modelscope_endpoint(
            base_url=args.base_url,
            api_key=args.api_key,
            model=args.model,
            system_prompt=args.system_prompt,
            prompt=final_prompt,
            image_paths=args.images,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
        ):
            print(delta, end="", flush=True)
            combined += delta
        if combined:
            print()
        else:
            print("Model returned an empty response.")
    else:
        response_text = call_modelscope_endpoint(
            base_url=args.base_url,
            api_key=args.api_key,
            model=args.model,
            system_prompt=args.system_prompt,
            prompt=final_prompt,
            image_paths=args.images,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
        )

        if response_text:
            print(response_text)
        else:
            print("Model returned an empty response.")


if __name__ == "__main__":
    main()
